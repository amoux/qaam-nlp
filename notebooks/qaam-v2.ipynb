{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Order in how each is method is called after loading text data and requesting an answer.\n",
    "\n",
    "```\n",
    "_load_path()\n",
    "_load_path()\n",
    "_load_path()\n",
    "add_url()\n",
    "texts_from_url()\n",
    "answer()\n",
    "_load_context()\n",
    "_load_spelling_context()\n",
    "_save_spelling_context()\n",
    "build_paragraph()\n",
    "max_model()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qaam_nlp import QAAM\n",
    "\n",
    "models = {\n",
    "    \"spacy-sm\": \"en_core_web_sm\",\n",
    "    \"spacy-lg\": \"en_core_web_lg\",\n",
    "    \"bert-uncased\": \"en_trf_bertbaseuncased_lg\"\n",
    "}\n",
    "\n",
    "qaam = QAAM(model=models[\"spacy-lg\"], top_k=15)\n",
    "qaam.add_url(\"http://dfaff921.ngrok.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of tokens extracted: 2840\n"
     ]
    }
   ],
   "source": [
    "# lets load the blog's texts to the model:\n",
    "qaam.texts_from_url(\"https://explosion.ai/blog/spacy-transformers\")\n",
    "print(f\"number of tokens extracted: {len(qaam.doc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Internally, the transformer model will predict over sentences',\n",
      " 'context': 'Internally, the transformer model will predict over sentences, '\n",
      "            'and the resulting tensor features will be reconstructed to '\n",
      "            'produce document-level annotations.'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "prediction = qaam.answer(\"How can I fine-tunme a Trnsformer model?\")\n",
    "pprint(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can I fine-tune a transformer model?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qaam.history[\"spelling\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'perform length-based subbatching internally',\n",
      " 'context': 'In order to further improve efficiency and reduce memory '\n",
      "            'requirements, we also perform length-based subbatching '\n",
      "            'internally. The aligned tokenization should be especially helpful '\n",
      "            'for answering questions like \"Do these two transformers pay '\n",
      "            'attention to the same words?\".'}\n"
     ]
    }
   ],
   "source": [
    "# QAAM has built it methods to handle spelling errors from a lg\n",
    "# english dictionary and the actual context of the texts extracted.\n",
    "pprint(qaam.answer(\"What to do to improev predictions?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What to do to improve predictions?']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qaam.history[\"spelling\"][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'a disadvantage when the model is small or data is limited',\n",
      " 'context': 'This (slightly more) \"blank slate\" approach is a disadvantage '\n",
      "            'when the model is small or data is limited, but with a big enough '\n",
      "            'model and sufficient examples, transformers are able to reach a '\n",
      "            'much more subtle understanding of linguistic information.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(qaam.answer(\"Is there anything wrong with this approach?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is there anything wrong with this approach?']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qaam.history[\"spelling\"][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6673790216445923\n"
     ]
    }
   ],
   "source": [
    "embed_question = qaam.embedd_sequence(qaam.history[\"spelling\"][1])\n",
    "embed_context = qaam.embedd_sequence(qaam.history[\"context\"][1])\n",
    "similary = qaam.cosine_distance(embed_question, embed_context)\n",
    "print(similary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'aligned tokenization',\n",
      " 'context': 'The aligned tokenization should be especially helpful for '\n",
      "            'answering questions like \"Do these two transformers pay attention '\n",
      "            'to the same words?\".'}\n"
     ]
    }
   ],
   "source": [
    "prediction = qaam.answer(\"How do models pay attention to same words in a sentence or context?\")\n",
    "pprint(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def similarities(doc_a, doc_b):\n",
    "    embed_a = qaam.embedd_sequence(doc_a)\n",
    "    embed_b = qaam.embedd_sequence(doc_b)\n",
    "    return qaam.cosine_distance(embed_a, embed_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "X = [0, 1, 2, 3] * 2\n",
    "Y = [1, 0, 1, 3] * 3\n",
    "random.shuffle(X)\n",
    "random.shuffle(Y)\n",
    "\n",
    "results = []\n",
    "for x, y in zip(X, Y):\n",
    "    (Q, C, A) = (\n",
    "        qaam.history['spelling'][x],\n",
    "        qaam.history['context'][y],\n",
    "        qaam.history[\"answer\"][y],)\n",
    "    S = similarities(Q, C)\n",
    "    results.append((Q, A, S))\n",
    "results = sorted(results, key=lambda i: i[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How can I fine-tune a transformer model?', 'Internally, the transformer model will predict over sentences', 0.6673790216445923)\n",
      "('How can I fine-tune a transformer model?', 'Internally, the transformer model will predict over sentences', 0.6673790216445923)\n",
      "('How can I fine-tune a transformer model?', 'Internally, the transformer model will predict over sentences', 0.6673790216445923)\n",
      "('How can I fine-tune a transformer model?', 'a disadvantage when the model is small or data is limited', 0.5869237780570984)\n",
      "('What to do to improve predictions?', 'Internally, the transformer model will predict over sentences', 0.5800701379776001)\n",
      "('Is there anything wrong with this approach?', 'Internally, the transformer model will predict over sentences', 0.4755496084690094)\n",
      "('Is there anything wrong with this approach?', 'Internally, the transformer model will predict over sentences', 0.4755496084690094)\n",
      "('What to do to improve predictions?', 'a disadvantage when the model is small or data is limited', 0.14870920777320862)\n"
     ]
    }
   ],
   "source": [
    "for doc in results: print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BERT', 6),\n",
       " ('NLP', 6),\n",
       " ('Prodigy', 4),\n",
       " ('2019', 4),\n",
       " ('API', 3),\n",
       " ('one', 3),\n",
       " ('two', 3),\n",
       " ('GPT-2', 2),\n",
       " ('GPU', 2),\n",
       " ('TPU', 2),\n",
       " ('Transformers', 2),\n",
       " ('IMDB', 2),\n",
       " ('‚ùÑ', 2),\n",
       " ('XLNet', 1),\n",
       " (\"Hugging Face's\", 1)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def get_blog_entities(spacy_doc: object, top_k=10):\n",
    "    entities = dict()\n",
    "    for ent in spacy_doc.ents:\n",
    "        ent = ent.text\n",
    "        if ent not in entities:\n",
    "            entities[ent] = 1\n",
    "        else:\n",
    "            entities[ent] += 1\n",
    "    return Counter(entities).most_common(top_k)\n",
    "\n",
    "get_blog_entities(qaam.doc, top_k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
